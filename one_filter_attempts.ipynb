{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following code is intended to process the road map images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access image filenames\n",
    "\n",
    "filenames = glob.glob('images/source_images/road_images/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num_Images: \",len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I will be using code found from [this](https://towardsdatascience.com/an-end-to-end-introduction-to-gans-bf253f1fa52f) GAN introduction to test the formatting of my images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Dropout\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras.backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.layers import Input, merge\n",
    "from keras.initializers import RandomNormal\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True \n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to normalize image pixels.\n",
    "def norm_img(img):\n",
    "    '''A function to Normalize Images.\n",
    "    Input:\n",
    "        img : Original image as numpy array.\n",
    "    Output: Normailized Image as numpy array\n",
    "    '''\n",
    "    img = (img / 127.5) - 1\n",
    "    return img\n",
    "\n",
    "def denorm_img(img):\n",
    "    '''A function to Denormailze, i.e. recreate image from normalized image\n",
    "    Input:\n",
    "        img : Normalized image as numpy array.\n",
    "    Output: Original Image as numpy array\n",
    "    '''\n",
    "    img = (img + 1) * 127.5\n",
    "    return img.astype(np.uint8) \n",
    "\n",
    "def sample_from_dataset(batch_size, image_shape, data_dir=None):\n",
    "    '''Create a batch of image samples by sampling random images from a data directory.\n",
    "    Resizes the image using image_shape and normalize the images.\n",
    "    Input:\n",
    "        batch_size : Sample size required\n",
    "        image_size : Size that Image should be resized to\n",
    "        data_dir : Path of directory where training images are placed.\n",
    "\n",
    "    Output:\n",
    "        sample : batch of processed images \n",
    "    '''\n",
    "    sample_dim = (batch_size,) + image_shape\n",
    "    sample = np.empty(sample_dim, dtype=np.float32)\n",
    "    all_data_dirlist = list(glob.glob(data_dir))\n",
    "    sample_imgs_paths = np.random.choice(all_data_dirlist,batch_size)\n",
    "    for index,img_filename in enumerate(sample_imgs_paths):\n",
    "        image = Image.open(img_filename)\n",
    "        image = image.resize(image_shape)\n",
    "        image = image.convert('1') ###CHANGED###\n",
    "        image = np.asarray(image)\n",
    "        image = norm_img(image)\n",
    "        sample[index,...] = image\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noise(batch_size, noise_shape):\n",
    "    ''' Generates a numpy vector sampled from normal distribution of shape (batch_size,noise_shape)\n",
    "    Input:\n",
    "        batch_size : size of batch\n",
    "        noise_shape: shape of noise vector, normally kept as 100 \n",
    "    Output:a numpy vector sampled from normal distribution of shape (batch_size,noise_shape)     \n",
    "    '''\n",
    "    return np.random.normal(0, 1, size=(batch_size,)+noise_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_normal(noise_shape):\n",
    "    ''' This function takes as input shape of the noise vector and creates the Keras generator    architecture.\n",
    "    '''\n",
    "    kernel_init = 'glorot_uniform'    \n",
    "    gen_input = Input(shape = noise_shape) \n",
    "    \n",
    "    # Transpose 2D conv layer 1. \n",
    "    generator = Conv2DTranspose(filters = 512, kernel_size = (4,4), strides = (1,1), padding = \"valid\", data_format = \"channels_last\", kernel_initializer = kernel_init)(gen_input)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    # Transpose 2D conv layer 2.\n",
    "    generator = Conv2DTranspose(filters = 256, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    # Transpose 2D conv layer 3.\n",
    "    generator = Conv2DTranspose(filters = 128, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    # Transpose 2D conv layer 4.\n",
    "    generator = Conv2DTranspose(filters = 64, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    # conv 2D layer 1.\n",
    "    generator = Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    # Final Transpose 2D conv layer 5 to generate final image. Filter size 3 for 3 image channel\n",
    "    generator = Conv2DTranspose(filters = 1, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    \n",
    "    # Tanh activation to get final normalized image\n",
    "    generator = Activation('tanh')(generator)\n",
    "    \n",
    "    # defining the optimizer and compiling the generator model.\n",
    "    gen_opt = Adam(lr=0.00015, beta_1=0.5)\n",
    "    generator_model = Model(input = gen_input, output = generator)\n",
    "    generator_model.compile(loss='binary_crossentropy', optimizer=gen_opt, metrics=['accuracy'])\n",
    "    generator_model.summary()\n",
    "    return generator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disc_normal(image_shape=(150,150)):\n",
    "    dropout_prob = 0.4\n",
    "    kernel_init = 'glorot_uniform'\n",
    "    dis_input = Input(shape = image_shape)\n",
    "    \n",
    "    # Conv layer 1:\n",
    "    discriminator = Conv2D(filters = 150, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(dis_input)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "    # Conv layer 2:\n",
    "    discriminator = Conv2D(filters = 300, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
    "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "    # Conv layer 3:   \n",
    "    discriminator = Conv2D(filters = 600, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
    "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "    # Conv layer 4:\n",
    "    discriminator = Conv2D(filters = 1200, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
    "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)#discriminator = MaxPooling2D(pool_size=(2, 2))(discriminator)\n",
    "    # Flatten\n",
    "    discriminator = Flatten()(discriminator)\n",
    "    # Dense Layer\n",
    "    discriminator = Dense(1)(discriminator)\n",
    "    # Sigmoid Activation\n",
    "    discriminator = Activation('sigmoid')(discriminator)\n",
    "    # Optimizer and Compiling model\n",
    "    dis_opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    discriminator_model = Model(input = dis_input, output = discriminator)\n",
    "    discriminator_model.compile(loss='binary_crossentropy', optimizer=dis_opt, metrics=['accuracy'])\n",
    "    discriminator_model.summary()\n",
    "    return discriminator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of noise vector to be input to the Generator\n",
    "noise_shape = (1,1,100)\n",
    "# Number of steps for training. num_epochs = num_steps*batch_size/data_size\n",
    "num_steps = 500\n",
    "# batch size for training.\n",
    "batch_size = 64\n",
    "# Location to save images and logs \n",
    "img_save_dir = \"images/gen_images/roads_2/\"\n",
    "# Image size to reshape to\n",
    "image_shape = (64,64) ###CHANGED###\n",
    "# Location of data directory\n",
    "data_dir = \"images/source_images/road_images/*\"\n",
    "# set up log and save directories\n",
    "log_dir = img_save_dir\n",
    "save_model_dir = img_save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img_batch(img_batch,img_save_dir):\n",
    "    '''Takes as input a image batch and a img_save_dir and saves 16 images from the batch in a 4x4 grid in the img_save_dir\n",
    "    '''\n",
    "    plt.figure(figsize=(16,16))\n",
    "    gs1 = gridspec.GridSpec(4, 4)\n",
    "    gs1.update(wspace=0, hspace=0)\n",
    "    rand_indices = np.random.choice(img_batch.shape[0],16,replace=False)\n",
    "    for i in range(16):\n",
    "        ax1 = plt.subplot(gs1[i])\n",
    "        ax1.set_aspect('equal')\n",
    "        rand_index = rand_indices[i]\n",
    "        image = img_batch[rand_index, :,:,:]\n",
    "        fig = plt.imshow(denorm_img(image))\n",
    "        plt.axis('off')\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_save_dir,bbox_inches='tight',pad_inches=0)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0525 12:12:45.444768 4643204544 deprecation_wrapper.py:119] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0525 12:12:45.448826 4643204544 deprecation_wrapper.py:119] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv2d_1: expected ndim=4, found ndim=3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-716898f9cd5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_disc_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gen_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9b3611eb921a>\u001b[0m in \u001b[0;36mget_disc_normal\u001b[0;34m(image_shape)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Conv layer 1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"same\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"channels_last\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Conv layer 2:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv2d_1: expected ndim=4, found ndim=3"
     ]
    }
   ],
   "source": [
    "discriminator = get_disc_normal(image_shape)\n",
    "generator = get_gen_normal(noise_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "\n",
    "# Optimizer for the GAN\n",
    "opt = Adam(lr=0.00015, beta_1=0.5) #same as generator\n",
    "# Input to the generator\n",
    "gen_inp = Input(shape=noise_shape)\n",
    "\n",
    "GAN_inp = generator(gen_inp)\n",
    "GAN_opt = discriminator(GAN_inp)\n",
    "\n",
    "# Final GAN\n",
    "gan = Model(input = gen_inp, output = GAN_opt)\n",
    "gan.compile(loss = 'binary_crossentropy', optimizer = opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a fixed noise vector to see how the GAN Images transition through time on a fixed noise. \n",
    "fixed_noise = gen_noise(16,noise_shape)\n",
    "\n",
    "# To keep Track of losses\n",
    "avg_disc_fake_loss = []\n",
    "avg_disc_real_loss = []\n",
    "avg_GAN_loss = []\n",
    "\n",
    "# We will run for num_steps iterations\n",
    "for step in range(num_steps): \n",
    "    tot_step = step\n",
    "    print(\"Begin step: \", tot_step)\n",
    "    # to keep track of time per step\n",
    "    step_begin_time = time.time() \n",
    "    \n",
    "    # sample a batch of normalized images from the dataset\n",
    "    real_data_X = sample_from_dataset(batch_size, image_shape, data_dir=data_dir)\n",
    "    \n",
    "    # Genearate noise to send as input to the generator\n",
    "    noise = gen_noise(batch_size,noise_shape)\n",
    "    \n",
    "    # Use generator to create(predict) images\n",
    "    fake_data_X = generator.predict(noise)\n",
    "    \n",
    "    # Save predicted images from the generator every 10th step\n",
    "    if (tot_step % 100) == 0:\n",
    "        step_num = str(tot_step).zfill(4)\n",
    "        save_img_batch(fake_data_X,img_save_dir+step_num+\"_image.png\")\n",
    "    \n",
    "    # Create the labels for real and fake data. We don't give exact ones and zeros but add a small amount of noise. This is an important GAN training trick\n",
    "    real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n",
    "    fake_data_Y = np.random.random_sample(batch_size)*0.2\n",
    "        \n",
    "    # train the discriminator using data and labels\n",
    "\n",
    "    discriminator.trainable = True\n",
    "    generator.trainable = False\n",
    "\n",
    "    # Training Discriminator seperately on real data\n",
    "    dis_metrics_real = discriminator.train_on_batch(real_data_X,real_data_Y) \n",
    "    # training Discriminator seperately on fake data\n",
    "    dis_metrics_fake = discriminator.train_on_batch(fake_data_X,fake_data_Y) \n",
    "    \n",
    "    print(\"Disc: real loss: %f fake loss: %f\" % (dis_metrics_real[0], dis_metrics_fake[0]))\n",
    "    \n",
    "    # Save the losses to plot later\n",
    "    avg_disc_fake_loss.append(dis_metrics_fake[0])\n",
    "    avg_disc_real_loss.append(dis_metrics_real[0])\n",
    "    \n",
    "    # Train the generator using a random vector of noise and its labels (1's with noise)\n",
    "    generator.trainable = True\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    GAN_X = gen_noise(batch_size,noise_shape)\n",
    "    GAN_Y = real_data_Y\n",
    "   \n",
    "    gan_metrics = gan.train_on_batch(GAN_X,GAN_Y)\n",
    "    print(\"GAN loss: %f\" % (gan_metrics[0]))\n",
    "    \n",
    "    # Log results by opening a file in append mode\n",
    "    text_file = open(log_dir+\"\\\\training_log.txt\", \"a\")\n",
    "    text_file.write(\"Step: %d Disc: real loss: %f fake loss: %f GAN loss: %f\\n\" % (tot_step, dis_metrics_real[0], dis_metrics_fake[0],gan_metrics[0]))\n",
    "    text_file.close()\n",
    "\n",
    "    # save GAN loss to plot later\n",
    "    avg_GAN_loss.append(gan_metrics[0])\n",
    "            \n",
    "    end_time = time.time()\n",
    "    diff_time = int(end_time - step_begin_time)\n",
    "    print(\"Step %d completed. Time took: %s secs.\" % (tot_step, diff_time))\n",
    "    \n",
    "    # save model at every 500 steps\n",
    "    if ((tot_step+1) % 500) == 0:\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Average Disc_fake loss: %f\" % (np.mean(avg_disc_fake_loss))) \n",
    "        print(\"Average Disc_real loss: %f\" % (np.mean(avg_disc_real_loss))) \n",
    "        print(\"Average GAN loss: %f\" % (np.mean(avg_GAN_loss)))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        discriminator.trainable = False\n",
    "        generator.trainable = False\n",
    "        # predict on fixed_noise\n",
    "        fixed_noise_generate = generator.predict(noise)\n",
    "        step_num = str(tot_step).zfill(4)\n",
    "        save_img_batch(fixed_noise_generate,img_save_dir+step_num+\"fixed_image.png\")\n",
    "        generator.save(save_model_dir+str(tot_step)+\"_GENERATOR_weights_and_arch.hdf5\")\n",
    "        discriminator.save(save_model_dir+str(tot_step)+\"_DISCRIMINATOR_weights_and_arch.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(generator, save_dir):\n",
    "    noise = gen_noise(batch_size,noise_shape)\n",
    "    fake_data_X = generator.predict(noise)\n",
    "    print(\"Displaying generated images\")\n",
    "    plt.figure(figsize=(16,16))\n",
    "    gs1 = gridspec.GridSpec(4, 4)\n",
    "    gs1.update(wspace=0, hspace=0)\n",
    "    rand_indices = np.random.choice(fake_data_X.shape[0],16,replace=False)\n",
    "    for i in range(16):\n",
    "        ax1 = plt.subplot(gs1[i])\n",
    "        ax1.set_aspect('equal')\n",
    "        rand_index = rand_indices[i]\n",
    "        image = fake_data_X[rand_index, :,:,:]\n",
    "        fig = plt.imshow(denorm_img(image))\n",
    "        plt.axis('off')\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir+str(time.time())+\"_GENERATEDimage.png\",bbox_inches='tight',pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "generate_images(generator, img_save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
