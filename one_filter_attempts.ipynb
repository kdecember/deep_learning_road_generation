{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following code is intended to process the road map images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access image filenames\n",
    "\n",
    "filenames = glob.glob('images/source_images/road_images/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_Images:  1108\n"
     ]
    }
   ],
   "source": [
    "print(\"Num_Images: \",len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I will be using code found from [this](https://towardsdatascience.com/an-end-to-end-introduction-to-gans-bf253f1fa52f) GAN introduction to test the formatting of my images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Dropout\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras.backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.layers import Input, merge\n",
    "from keras.initializers import RandomNormal\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True \n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to normalize image pixels.\n",
    "def norm_img(img):\n",
    "    '''A function to Normalize Images.\n",
    "    Input:\n",
    "        img : Original image as numpy array.\n",
    "    Output: Normailized Image as numpy array\n",
    "    '''\n",
    "    img = (img / 127.5) - 1\n",
    "    return img\n",
    "\n",
    "def denorm_img(img):\n",
    "    '''A function to Denormailze, i.e. recreate image from normalized image\n",
    "    Input:\n",
    "        img : Normalized image as numpy array.\n",
    "    Output: Original Image as numpy array\n",
    "    '''\n",
    "    img = (img + 1) * 127.5\n",
    "    return img.astype(np.uint8) \n",
    "\n",
    "def sample_from_dataset(batch_size, image_shape, data_dir=None):\n",
    "    '''Create a batch of image samples by sampling random images from a data directory.\n",
    "    Resizes the image using image_shape and normalize the images.\n",
    "    Input:\n",
    "        batch_size : Sample size required\n",
    "        image_size : Size that Image should be resized to\n",
    "        data_dir : Path of directory where training images are placed.\n",
    "\n",
    "    Output:\n",
    "        sample : batch of processed images \n",
    "    '''\n",
    "    sample_dim = (batch_size,) + image_shape\n",
    "    sample = np.empty(sample_dim, dtype=np.float32)\n",
    "    all_data_dirlist = list(glob.glob(data_dir))\n",
    "    sample_imgs_paths = np.random.choice(all_data_dirlist,batch_size)\n",
    "    for index,img_filename in enumerate(sample_imgs_paths):\n",
    "        image = Image.open(img_filename)\n",
    "        image = image.resize(image_shape[:-1])\n",
    "        image = image.convert('1') ###CHANGED###\n",
    "        image = np.asarray(image)\n",
    "        image = norm_img(image)\n",
    "        sample[index,...] = image ###CHANGED###\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noise(batch_size, noise_shape):\n",
    "    ''' Generates a numpy vector sampled from normal distribution of shape (batch_size,noise_shape)\n",
    "    Input:\n",
    "        batch_size : size of batch\n",
    "        noise_shape: shape of noise vector, normally kept as 100 \n",
    "    Output:a numpy vector sampled from normal distribution of shape (batch_size,noise_shape)     \n",
    "    '''\n",
    "    return np.random.normal(0, 1, size=(batch_size,)+noise_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_normal(noise_shape):\n",
    "    ''' This function takes as input shape of the noise vector and creates the Keras generator    architecture.\n",
    "    '''\n",
    "    kernel_init = 'glorot_uniform'    \n",
    "    gen_input = Input(shape = noise_shape) \n",
    "    \n",
    "    # Transpose 2D conv layer 1. \n",
    "    generator = Conv2DTranspose(filters = 512, kernel_size = (4,4), strides = (1,1), padding = \"valid\", data_format = \"channels_last\", kernel_initializer = kernel_init)(gen_input)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    # Transpose 2D conv layer 2.\n",
    "    generator = Conv2DTranspose(filters = 256, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    # Transpose 2D conv layer 3.\n",
    "    generator = Conv2DTranspose(filters = 128, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    # Transpose 2D conv layer 4.\n",
    "    generator = Conv2DTranspose(filters = 64, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    # conv 2D layer 1.\n",
    "    generator = Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    # Final Transpose 2D conv layer 5 to generate final image. Filter size 3 for 3 image channel\n",
    "    generator = Conv2DTranspose(filters = 1, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    \n",
    "    # Tanh activation to get final normalized image\n",
    "    generator = Activation('tanh')(generator)\n",
    "    \n",
    "    # defining the optimizer and compiling the generator model.\n",
    "    gen_opt = Adam(lr=0.00015, beta_1=0.5)\n",
    "    generator_model = Model(input = gen_input, output = generator)\n",
    "    generator_model.compile(loss='binary_crossentropy', optimizer=gen_opt, metrics=['accuracy'])\n",
    "    generator_model.summary()\n",
    "    return generator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disc_normal(image_shape=(64,64,3)):\n",
    "    dropout_prob = 0.4\n",
    "    kernel_init = 'glorot_uniform'\n",
    "    dis_input = Input(shape = image_shape)\n",
    "    \n",
    "    # Conv layer 1:\n",
    "    discriminator = Conv2D(filters = 64, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(dis_input)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "    # Conv layer 2:\n",
    "    discriminator = Conv2D(filters = 128, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
    "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "    # Conv layer 3:   \n",
    "    discriminator = Conv2D(filters = 256, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
    "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "    # Conv layer 4:\n",
    "    discriminator = Conv2D(filters = 512, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
    "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)#discriminator = MaxPooling2D(pool_size=(2, 2))(discriminator)\n",
    "    # Flatten\n",
    "    discriminator = Flatten()(discriminator)\n",
    "    # Dense Layer\n",
    "    discriminator = Dense(1)(discriminator)\n",
    "    # Sigmoid Activation\n",
    "    discriminator = Activation('sigmoid')(discriminator)\n",
    "    # Optimizer and Compiling model\n",
    "    dis_opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    discriminator_model = Model(input = dis_input, output = discriminator)\n",
    "    discriminator_model.compile(loss='binary_crossentropy', optimizer=dis_opt, metrics=['accuracy'])\n",
    "    discriminator_model.summary()\n",
    "    return discriminator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of noise vector to be input to the Generator\n",
    "noise_shape = (1,1,100)\n",
    "# Number of steps for training. num_epochs = num_steps*batch_size/data_size\n",
    "num_steps = 500\n",
    "# batch size for training.\n",
    "batch_size = 64\n",
    "# Location to save images and logs \n",
    "img_save_dir = \"images/gen_images/roads_2/\"\n",
    "# Image size to reshape to\n",
    "image_shape = (64,64) ###CHANGED###\n",
    "# Location of data directory\n",
    "data_dir = \"images/source_images/road_images/*\"\n",
    "# set up log and save directories\n",
    "log_dir = img_save_dir\n",
    "save_model_dir = img_save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img_batch(img_batch,img_save_dir):\n",
    "    '''Takes as input a image batch and a img_save_dir and saves 16 images from the batch in a 4x4 grid in the img_save_dir\n",
    "    '''\n",
    "    plt.figure(figsize=(16,16))\n",
    "    gs1 = gridspec.GridSpec(4, 4)\n",
    "    gs1.update(wspace=0, hspace=0)\n",
    "    rand_indices = np.random.choice(img_batch.shape[0],16,replace=False)\n",
    "    for i in range(16):\n",
    "        ax1 = plt.subplot(gs1[i])\n",
    "        ax1.set_aspect('equal')\n",
    "        rand_index = rand_indices[i]\n",
    "        image = img_batch[rand_index, :,:,:]\n",
    "        fig = plt.imshow(denorm_img(image))\n",
    "        plt.axis('off')\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_save_dir,bbox_inches='tight',pad_inches=0)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0525 11:54:53.071076 4551634368 deprecation_wrapper.py:119] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0525 11:54:53.074221 4551634368 deprecation_wrapper.py:119] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0525 11:54:53.080255 4551634368 deprecation_wrapper.py:119] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0525 11:54:53.153159 4551634368 deprecation_wrapper.py:119] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0525 11:54:53.154247 4551634368 deprecation_wrapper.py:119] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0525 11:54:53.202586 4551634368 deprecation_wrapper.py:119] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "/Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n",
      "W0525 11:54:53.606336 4551634368 deprecation_wrapper.py:119] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0525 11:54:53.618586 4551634368 deprecation.py:323] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1088      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 128)       131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 256)         524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 512)         2097664   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,766,273\n",
      "Trainable params: 2,764,481\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 4, 4, 512)         819712    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 8, 8, 256)         2097408   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 16, 16, 128)       524416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 32, 32, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 64, 64, 1)         1025      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 64, 1)         0         \n",
      "=================================================================\n",
      "Total params: 3,614,721\n",
      "Trainable params: 3,612,673\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    }
   ],
   "source": [
    "discriminator = get_disc_normal(image_shape)\n",
    "generator = get_gen_normal(noise_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"mo...)`\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "discriminator.trainable = False\n",
    "\n",
    "# Optimizer for the GAN\n",
    "opt = Adam(lr=0.00015, beta_1=0.5) #same as generator\n",
    "# Input to the generator\n",
    "gen_inp = Input(shape=noise_shape)\n",
    "\n",
    "GAN_inp = generator(gen_inp)\n",
    "GAN_opt = discriminator(GAN_inp)\n",
    "\n",
    "# Final GAN\n",
    "gan = Model(input = gen_inp, output = GAN_opt)\n",
    "gan.compile(loss = 'binary_crossentropy', optimizer = opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin step:  0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 1 must be sequence of length 2, not 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-358f132a6b23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# sample a batch of normalized images from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mreal_data_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Genearate noise to send as input to the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-65c0e3b5c038>\u001b[0m in \u001b[0;36msample_from_dataset\u001b[0;34m(batch_size, image_shape, data_dir)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_filename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_imgs_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m###CHANGED###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m###CHANGED###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box)\u001b[0m\n\u001b[1;32m   1804\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m     def rotate(self, angle, resample=NEAREST, expand=0, center=None,\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 1 must be sequence of length 2, not 3"
     ]
    }
   ],
   "source": [
    "# Use a fixed noise vector to see how the GAN Images transition through time on a fixed noise. \n",
    "fixed_noise = gen_noise(16,noise_shape)\n",
    "\n",
    "# To keep Track of losses\n",
    "avg_disc_fake_loss = []\n",
    "avg_disc_real_loss = []\n",
    "avg_GAN_loss = []\n",
    "\n",
    "# We will run for num_steps iterations\n",
    "for step in range(num_steps): \n",
    "    tot_step = step\n",
    "    print(\"Begin step: \", tot_step)\n",
    "    # to keep track of time per step\n",
    "    step_begin_time = time.time() \n",
    "    \n",
    "    # sample a batch of normalized images from the dataset\n",
    "    real_data_X = sample_from_dataset(batch_size, image_shape, data_dir=data_dir)\n",
    "    \n",
    "    # Genearate noise to send as input to the generator\n",
    "    noise = gen_noise(batch_size,noise_shape)\n",
    "    \n",
    "    # Use generator to create(predict) images\n",
    "    fake_data_X = generator.predict(noise)\n",
    "    \n",
    "    # Save predicted images from the generator every 10th step\n",
    "    if (tot_step % 100) == 0:\n",
    "        step_num = str(tot_step).zfill(4)\n",
    "        save_img_batch(fake_data_X,img_save_dir+step_num+\"_image.png\")\n",
    "    \n",
    "    # Create the labels for real and fake data. We don't give exact ones and zeros but add a small amount of noise. This is an important GAN training trick\n",
    "    real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n",
    "    fake_data_Y = np.random.random_sample(batch_size)*0.2\n",
    "        \n",
    "    # train the discriminator using data and labels\n",
    "\n",
    "    discriminator.trainable = True\n",
    "    generator.trainable = False\n",
    "\n",
    "    # Training Discriminator seperately on real data\n",
    "    dis_metrics_real = discriminator.train_on_batch(real_data_X,real_data_Y) \n",
    "    # training Discriminator seperately on fake data\n",
    "    dis_metrics_fake = discriminator.train_on_batch(fake_data_X,fake_data_Y) \n",
    "    \n",
    "    print(\"Disc: real loss: %f fake loss: %f\" % (dis_metrics_real[0], dis_metrics_fake[0]))\n",
    "    \n",
    "    # Save the losses to plot later\n",
    "    avg_disc_fake_loss.append(dis_metrics_fake[0])\n",
    "    avg_disc_real_loss.append(dis_metrics_real[0])\n",
    "    \n",
    "    # Train the generator using a random vector of noise and its labels (1's with noise)\n",
    "    generator.trainable = True\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    GAN_X = gen_noise(batch_size,noise_shape)\n",
    "    GAN_Y = real_data_Y\n",
    "   \n",
    "    gan_metrics = gan.train_on_batch(GAN_X,GAN_Y)\n",
    "    print(\"GAN loss: %f\" % (gan_metrics[0]))\n",
    "    \n",
    "    # Log results by opening a file in append mode\n",
    "    text_file = open(log_dir+\"\\\\training_log.txt\", \"a\")\n",
    "    text_file.write(\"Step: %d Disc: real loss: %f fake loss: %f GAN loss: %f\\n\" % (tot_step, dis_metrics_real[0], dis_metrics_fake[0],gan_metrics[0]))\n",
    "    text_file.close()\n",
    "\n",
    "    # save GAN loss to plot later\n",
    "    avg_GAN_loss.append(gan_metrics[0])\n",
    "            \n",
    "    end_time = time.time()\n",
    "    diff_time = int(end_time - step_begin_time)\n",
    "    print(\"Step %d completed. Time took: %s secs.\" % (tot_step, diff_time))\n",
    "    \n",
    "    # save model at every 500 steps\n",
    "    if ((tot_step+1) % 500) == 0:\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Average Disc_fake loss: %f\" % (np.mean(avg_disc_fake_loss))) \n",
    "        print(\"Average Disc_real loss: %f\" % (np.mean(avg_disc_real_loss))) \n",
    "        print(\"Average GAN loss: %f\" % (np.mean(avg_GAN_loss)))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        discriminator.trainable = False\n",
    "        generator.trainable = False\n",
    "        # predict on fixed_noise\n",
    "        fixed_noise_generate = generator.predict(noise)\n",
    "        step_num = str(tot_step).zfill(4)\n",
    "        save_img_batch(fixed_noise_generate,img_save_dir+step_num+\"fixed_image.png\")\n",
    "        generator.save(save_model_dir+str(tot_step)+\"_GENERATOR_weights_and_arch.hdf5\")\n",
    "        discriminator.save(save_model_dir+str(tot_step)+\"_DISCRIMINATOR_weights_and_arch.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(generator, save_dir):\n",
    "    noise = gen_noise(batch_size,noise_shape)\n",
    "    fake_data_X = generator.predict(noise)\n",
    "    print(\"Displaying generated images\")\n",
    "    plt.figure(figsize=(16,16))\n",
    "    gs1 = gridspec.GridSpec(4, 4)\n",
    "    gs1.update(wspace=0, hspace=0)\n",
    "    rand_indices = np.random.choice(fake_data_X.shape[0],16,replace=False)\n",
    "    for i in range(16):\n",
    "        ax1 = plt.subplot(gs1[i])\n",
    "        ax1.set_aspect('equal')\n",
    "        rand_index = rand_indices[i]\n",
    "        image = fake_data_X[rand_index, :,:,:]\n",
    "        fig = plt.imshow(denorm_img(image))\n",
    "        plt.axis('off')\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir+str(time.time())+\"_GENERATEDimage.png\",bbox_inches='tight',pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "generate_images(generator, img_save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
